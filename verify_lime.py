import argparse

# command-line arguments.
parser = argparse.ArgumentParser(description=__doc__)
parser.add_argument(
    'image_path', type=str,
    help='image on which to carry out inference and prediction')
parser.add_argument(
    '--classes', type=int, default=1,
    help='provide explanations for the top n classes predicted for the given image')
parser.add_argument(
    '--perturbations', type=int, default=1000,
    help='number of perturbations to generate for the given image')
parser.add_argument(
    '--batch-size', type=int, default=256,
    help='batch size to be used while passing perturbations to the AlexNet model')
parser.add_argument(
    '--segments', type=int, default=5,
    help='number of segments to be shown in the explanation')
parser.add_argument(
    '--hide-colour', type=str, choices=['black', 'white', 'mean'], default='mean',
    help='colour used for hiding segments while generating perturbations')
args = parser.parse_args()

# # Using Lime with Pytorch

# In this tutorial we will show how to use Lime framework with Pytorch. Specifically, we will use Lime to explain the prediction generated by one of the pretrained ImageNet models.
# 
# Let's start with importing our dependencies. This code is tested with Pytorch 1.0 but should work with older versions as well.

import matplotlib.pyplot as plt
from PIL import Image
import torch.nn as nn
import numpy as np
import os, json

import torch
from torchvision import models, transforms
from torch.autograd import Variable
import torch.nn.functional as F

# Load our test image and see how it looks.

def get_image(path):
    with open(os.path.abspath(path), 'rb') as f:
        with Image.open(f) as img:
            return img.convert('RGB') 
        
img = get_image(args.image_path)

# We need to convert this image to Pytorch tensor and also apply whitening as used by our pretrained model.

# resize and take the center part of image to what our model expects
def get_input_transform():
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                    std=[0.229, 0.224, 0.225])       
    transf = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        normalize
    ])    

    return transf

def get_input_tensors(img):
    transf = get_input_transform()
    # unsqeeze converts single image to batch of 1
    return transf(img).unsqueeze(0)

# Load the pretrained model for Resnet50 available in Pytorch.

model = models.alexnet(pretrained=True)

# Load label texts for ImageNet predictions so we know what model is predicting

idx2label, cls2label, cls2idx = [], {}, {}
with open('wnids_1000.txt', 'r') as f:
    for i, line in enumerate(f.readlines()):
        name, wordnet_id = line.split(',')
        idx2label.append(name)
        cls2label[wordnet_id] = name
        cls2idx[name] = i

# Get the predicition for our image.

img_t = get_input_tensors(img)
model.eval()
logits = model(img_t)

# Predicitions we got are logits. Let's pass that through softmax to get probabilities and
# class labels for top n predictions where n = args.classes.

probs = F.softmax(logits, dim=1)
probs5 = probs.topk(args.classes)
print(tuple((p,c, idx2label[c]) for p, c in zip(probs5[0][0].detach().numpy(),
    probs5[1][0].detach().numpy())))

# We are getting ready to use Lime. Lime produces the array of images from original input image by pertubation algorithm. So we need to provide two things: (1) original image as numpy array (2) classification function that would take array of purturbed images as input and produce the probabilities for each class for each image as output. 
# 
# For Pytorch, first we need to define two separate transforms: (1) to take PIL image, resize and crop it (2) take resized, cropped image and apply whitening.

def get_pil_transform(): 
    transf = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.CenterCrop(224)
    ])    

    return transf

def get_preprocess_transform():
    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],
                                    std=[0.229, 0.224, 0.225])     
    transf = transforms.Compose([
        transforms.ToTensor(),
        normalize
    ])    

    return transf    

pill_transf = get_pil_transform()
preprocess_transform = get_preprocess_transform()

# Now we are ready to define classification function that Lime needs. The input to this function is numpy array of images where each image is ndarray of shape (channel, height, width). The output is numpy aaray of shape (image index, classes) where each value in array should be probability for that image, class combination.

def batch_predict(images):
    model.eval()
    batch = torch.stack(tuple(preprocess_transform(i) for i in images), dim=0)

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    batch = batch.to(device)
    
    logits = model(batch)
    probs = F.softmax(logits, dim=1)
    return probs.detach().cpu().numpy()

# Let's test our function for the sample image.

test_pred = batch_predict([pill_transf(img)])
print(test_pred.squeeze().argmax())

# Import lime and create explanation for this prediciton.

from lime import lime_image

if args.hide_colour == 'black':
    hide_colour = 0
elif args.hide_colour == 'white':
    hide_colour = 255
else:
    hide_colour = None

explainer = lime_image.LimeImageExplainer()
explanation = explainer.explain_instance(np.array(pill_transf(img)), 
                                         batch_predict, # classification function
                                         top_labels=args.classes, 
                                         hide_color=hide_colour, 
                                         batch_size=args.batch_size,
                                         num_samples=args.perturbations) # number of images that will be sent to classification function

# Let's turn on areas that contributes against the top prediction.

from skimage.segmentation import mark_boundaries

for i in range(args.classes):
    temp, mask = explanation.get_image_and_mask(explanation.top_labels[i],
            positive_only=False, num_features=args.segments, hide_rest=False)
    img_boundry2 = mark_boundaries(temp/255.0, mask)
    plt.imshow(img_boundry2)
    file = 'lime_pkg_explanation_class{}.png'.format(i+1)
    plt.savefig(file)
    print('Explanation saved to', file)
